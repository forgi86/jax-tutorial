{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b4fe5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.random as jr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da917af",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32\n",
    "output_size = 10\n",
    "K = 100 # K-shot, number of examples per task\n",
    "batch_size = 32 # meta batch size\n",
    "alpha = 0.1 # inner learning rate\n",
    "lr = 0.001 # outer learning rate\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9d64d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.key(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c548ec4",
   "metadata": {},
   "source": [
    "## Jax tutorial: Meta Learning with HyperNets\n",
    "\n",
    "In JAX, it's straightforward to sketch a HyperNet-based meta learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25415754",
   "metadata": {},
   "source": [
    "Let us first define a meta-batch for meta-learning, having a static regression problem in mind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "012d7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The meta batch: batch_size static regression datasets\n",
    "batch_x = onp.random.randn(batch_size, K, input_size)\n",
    "batch_y = onp.random.randn(batch_size, K, output_size)\n",
    "\n",
    "# support set, aka context, training set\n",
    "batch_x1 = batch_x[:, :K//2]\n",
    "batch_y1 = batch_y[:, :K//2]\n",
    "# query set, test set\n",
    "batch_x2 = batch_x[:, K//2:]\n",
    "batch_y2 = batch_y[:, K//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25e274",
   "metadata": {},
   "source": [
    "The (meta) batch consists in ``batch_size`` input-output pairs of ``K`` elements each. \n",
    "\n",
    "The idea is that ``(batch_x[i], batch_y[i])`` is a dataset with ``K`` samples from the **same** data-generating system. \n",
    "Conversely, ``(batch_x[i], batch_y[i])`` ``(batch_x[i], batch_y[i])``, for ``i  ~= j`` are two datasets from different, yet **related** data-generating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edb2fd",
   "metadata": {},
   "source": [
    "Let us define a simple MLP as base architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "894bdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple MLP (stock code from copilot)\n",
    "class MLP(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_size)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf05e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP(hidden_size=128, output_size=output_size)\n",
    "key, subkey = jr.split(key)\n",
    "x = jnp.ones((input_size))  # Example input with 32 features\n",
    "params = mlp.init(subkey, x)  # Initialize parameters\n",
    "params_mlp_hn = mlp.apply(params, x)  # Forward pass\n",
    "params_mlp_hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b130bc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5514"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_flat, unflatten_params_fn = jax.flatten_util.ravel_pytree(params)\n",
    "n_params = params_flat.size\n",
    "n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7116eb",
   "metadata": {},
   "source": [
    "The hypernet takes in a dataset and generates corresponding mlp model parameters. It is kind of a learned algorithm. Considering a case of static regression, we use a deep set hypernetsince it is permutation-invariant, like the algorithm we aim to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0e882f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Apply a shared MLP to each element in the set\n",
    "        x = nn.Dense(self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        # Aggregate the set using a permutation-invariant operation (e.g., sum)\n",
    "        x = jnp.sum(x, axis=-2)\n",
    "\n",
    "        # Apply another MLP to the aggregated representation\n",
    "        x = nn.Dense(self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_size)(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43afb205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5514,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "hypernet = DeepSet(hidden_size=128, output_size=n_params)\n",
    "key, subkey = jr.split(key)\n",
    "xy_input = jnp.ones((K, input_size + output_size))  # The hypernet processes K input-output pairs...\n",
    "params_hypernet = hypernet.init(subkey, xy_input)  # Initialize parameters\n",
    "params_mlp_hn = hypernet.apply(params_hypernet, xy_input)  # And splits out the mlp params\n",
    "params_mlp_hn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54afbb2",
   "metadata": {},
   "source": [
    "The standard regression loss, boring stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19f6711c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.495289, dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(params, x, y):\n",
    "    pred = mlp.apply(params, x)\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "loss_fn(params, batch_x[0], batch_y[0])  # Loss for the first task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a477be",
   "metadata": {},
   "source": [
    "The hypernet loss, slightly more interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f9af2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(20384282., dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hypernet_loss(ph, x1, y1, x2, y2):\n",
    "\n",
    "\n",
    "    # Generate the weights using the hypernetwork\n",
    "    x1y1 = jnp.concatenate((x1, y1), axis=-1)  # Concatenate x1 and y2\n",
    "    weights = hypernet.apply(ph, x1y1)\n",
    "\n",
    "    # Unflatten the weights to match the model's parameters\n",
    "    pm = unflatten_params_fn(weights)\n",
    "\n",
    "    return loss_fn(pm, x2, y2)  # Loss for the second task\n",
    "\n",
    "hypernet_loss(params_hypernet, batch_x1[0], batch_y1[0], batch_x2[0], batch_y2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d91ce4",
   "metadata": {},
   "source": [
    "Let us vectorize the hypernet loss to make it amenable for \"meta mini-batch training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08bbd1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(20158264., dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_hypernet_loss(ph, x1_b, y1_b, x2_b, y2_b):\n",
    "    hn_loss_cfg = partial(hypernet_loss, ph) # fix the first argument\n",
    "    hn_loss_vmapped = jax.vmap(hn_loss_cfg) # vmap over the rest\n",
    "    task_losses = hn_loss_vmapped(x1_b, y1_b, x2_b, y2_b)\n",
    "    return jnp.mean(task_losses)\n",
    "\n",
    "batched_hypernet_loss(params_hypernet, batch_x1, batch_y1, batch_x2, batch_y2)  # Inner update for the first task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3160d97",
   "metadata": {},
   "source": [
    "Note: the initial loss is poorly scaled. We should have scaled the hypernet to split out reasonable mlp parameters at initialization..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8619e9",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "See the full [hypernet meta learning example](gallery/hypernet_sines.ipynb) in the gallery!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
