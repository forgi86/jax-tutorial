{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4fe5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.random as jr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da917af",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32\n",
    "output_size = 10\n",
    "K = 100 # K-shot, number of examples per task\n",
    "batch_size = 32 # meta batch size\n",
    "alpha = 0.1 # inner learning rate\n",
    "lr = 0.001 # outer learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c548ec4",
   "metadata": {},
   "source": [
    "## Jax tutorial: Meta Learning with MAML\n",
    "\n",
    "I't straightforward to sketch a model-based Meta Learning algorithm in JAX such as MAML without any extra dependency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25415754",
   "metadata": {},
   "source": [
    "Let us first define a meta-batch for meta-learning, having a static regression problem in mind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "012d7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The meta batch\n",
    "batch_x = onp.random.randn(batch_size, K, input_size)\n",
    "batch_y = onp.random.randn(batch_size, K, output_size)\n",
    "\n",
    "# support set, aka context, training set\n",
    "batch_x1 = batch_x[:, :K//2]\n",
    "batch_y1 = batch_y[:, :K//2]\n",
    "# query set, test set\n",
    "batch_x2 = batch_x[:, K//2:]\n",
    "batch_y2 = batch_y[:, K//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25e274",
   "metadata": {},
   "source": [
    "The (meta) batch consists in ``batch_size`` input-output pairs of ``K`` elements each. \n",
    "\n",
    "The idea is that ``(batch_x[i], batch_y[i])`` is a dataset with ``K`` samples from the **same** data-generating system. \n",
    "Conversely, ``(batch_x[i], batch_y[i])`` ``(batch_x[i], batch_y[i])``, for ``i  ~= j`` are two datasets from different, yet **related** data-generating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edb2fd",
   "metadata": {},
   "source": [
    "Let us define a simple MLP as base architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894bdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple MLP (stock code from copilot)\n",
    "class MLP(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_size)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf05e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP(hidden_size=128, output_size=output_size)\n",
    "key = jr.PRNGKey(0)\n",
    "x = jnp.ones((input_size))  # Example input with 32 features\n",
    "params = mlp.init(key, x)  # Initialize parameters\n",
    "output = mlp.apply(params, x)  # Forward pass\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43207d",
   "metadata": {},
   "source": [
    "Let us build the MAML loss incrementally. Taking advantage of JAX's transforms, first we define the loss for a single instance in the meta-dataset, and then vectorize it with a ``vmap`` transform. This, in my opinion, maximizes readability of the code.\n",
    "\n",
    "The first step is to define the standard regression loss for a single dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2f91d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.5897797, dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(params, x, y):\n",
    "    pred = mlp.apply(params, x)\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "loss_fn(params, batch_x[0], batch_y[0])  # Loss for the first task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b0860",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "Then, we define the MAML inner update step, namely a gradient descent step. Pretty straightforward in plain JAX!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98091fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAML inner update\n",
    "def inner_update(p, x1, y1):\n",
    "    grads = jax.grad(loss_fn)(p, x1, y1)\n",
    "    inner_sgd_fn = lambda g, state: (state - alpha*g) # GD update\n",
    "    return jax.tree_util.tree_map(inner_sgd_fn, grads, p)\n",
    "\n",
    "params_updated = inner_update(params, batch_x1[0], batch_y1[0])  # Inner update for the first task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ae750",
   "metadata": {},
   "source": [
    "The MAML loss is the regression loss ``loss_fn`` measured on ``(x2, y2)`` with parameters updated with GD executed on ``(x1, y1)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3af6332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.4345717, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maml_loss(p, x1, y1, x2, y2):\n",
    "    p2 = inner_update(p, x1, y1)\n",
    "    return loss_fn(p2, x2, y2)\n",
    "\n",
    "maml_loss(params, batch_x1[0], batch_y1[0], batch_x2[0], batch_y2[0])  # Inner update for the first task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c2581",
   "metadata": {},
   "source": [
    "The ``maml_loss`` defined above only handles a single instance in the meta-dataset. Let us batchify it with ``vmap``!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2eee0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.4123442, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batched maml loss\n",
    "def batched_maml_loss(p, x1_b, y1_b, x2_b, y2_b):\n",
    "    maml_loss_cfg = partial(maml_loss, p) # fix first argument\n",
    "    maml_loss_batch = jax.vmap(maml_loss_cfg) # vmap over the rest\n",
    "    task_losses = maml_loss_batch(x1_b, y1_b, x2_b, y2_b)\n",
    "    #maml_loss_batch = jax.vmap(maml_loss, in_axes=(None, 0, 0, 0, 0)) # alternative\n",
    "    #task_losses = maml_loss_batch(p, x1_b, y1_b, x2_b, y2_b)\n",
    "    return jnp.mean(task_losses)\n",
    "\n",
    "batched_maml_loss(params, batch_x1, batch_y1, batch_x2, batch_y2)  # Inner update for the first task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0625e",
   "metadata": {},
   "source": [
    "Voila'! This is the meta-training loss we wanna minimize wrt ``params``. Gradients are there for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12bc150",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grads = jax.value_and_grad(batched_maml_loss)(params, batch_x1, batch_y1, batch_x2, batch_y2)  # Inner update for the first task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f224c",
   "metadata": {},
   "source": [
    "The actual MAML training loop may look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee92542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_optimizer = optax.adam(learning_rate=lr)\n",
    "meta_opt_state = meta_optimizer.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def make_step(p, s, x1, y1, x2, y2):\n",
    "    l, g = jax.value_and_grad(batched_maml_loss)(p, x1, y1, x2, y2)\n",
    "    u, s = meta_optimizer.update(g, s)\n",
    "    p = optax.apply_updates(p, u)\n",
    "    return p, s, l\n",
    "\n",
    "\n",
    "losses = []\n",
    "for i in range(100):\n",
    "    #batch_x1, batch_y1, batch_x2, batch_y2 = sample_tasks()\n",
    "    params, meta_opt_state, loss = make_step(\n",
    "        params, meta_opt_state, batch_x1, batch_y1, batch_x2, batch_y2\n",
    "    )\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e92dc",
   "metadata": {},
   "source": [
    "See the full [maml meta learning example](gallery/maml_sines.ipynb) in the gallery!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
